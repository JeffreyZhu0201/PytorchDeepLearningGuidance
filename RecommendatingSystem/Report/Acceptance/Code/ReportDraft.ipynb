{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f80a0f7",
   "metadata": {},
   "source": [
    "# Deep Learning Recommenddation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa87976",
   "metadata": {},
   "source": [
    "## 数据集准备\n",
    "基于Movie-Lens 32M数据集进行实验\n",
    "*ml-32m*\n",
    "* ml-32m/ratings.csv\n",
    "* ml-32m/movies.csv\n",
    "* ml-32m/tags.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83430a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ml-32m/ratings.csv\\nuserId,movieId,rating,timestamp\\n1,17,4.0,944249077\\n1,25,1.0,944250228\\n1,29,2.0,943230976\\n1,30,5.0,944249077\\n# ml-32m/movies.csv\\nmovieId,title,genres\\n1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\\n2,Jumanji (1995),Adventure|Children|Fantasy\\n3,Grumpier Old Men (1995),Comedy|Romance\\n4,Waiting to Exhale (1995),Comedy|Drama|Romance\\n5,Father of the Bride Part II (1995),Comedy\\n# ml-32m/tags.csv\\nuserId,movieId,tag,timestamp\\n22,26479,Kevin Kline,1583038886\\n22,79592,misogyny,1581476297\\n22,247150,acrophobia,1622483469\\n34,2174,music,1249808064\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ml-32m/ratings.csv\n",
    "userId,movieId,rating,timestamp\n",
    "1,17,4.0,944249077\n",
    "1,25,1.0,944250228\n",
    "1,29,2.0,943230976\n",
    "1,30,5.0,944249077\n",
    "# ml-32m/movies.csv\n",
    "movieId,title,genres\n",
    "1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\n",
    "2,Jumanji (1995),Adventure|Children|Fantasy\n",
    "3,Grumpier Old Men (1995),Comedy|Romance\n",
    "4,Waiting to Exhale (1995),Comedy|Drama|Romance\n",
    "5,Father of the Bride Part II (1995),Comedy\n",
    "# ml-32m/tags.csv\n",
    "userId,movieId,tag,timestamp\n",
    "22,26479,Kevin Kline,1583038886\n",
    "22,79592,misogyny,1581476297\n",
    "22,247150,acrophobia,1622483469\n",
    "34,2174,music,1249808064\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccfbef",
   "metadata": {},
   "source": [
    "## 基础NCF模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e266b52",
   "metadata": {},
   "source": [
    "#### 导入必要包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2331da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f725cd1",
   "metadata": {},
   "source": [
    "#### 加载设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a509f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用GPU加速\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型和优化器\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if(torch.cuda.is_available()):\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"使用GPU加速\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271cb8c",
   "metadata": {},
   "source": [
    "#### 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb6d10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据读取成功\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理\n",
    "ratings = pd.read_csv('../Dataset/ml-32m/ratings.csv')\n",
    "csv_file_path = 'loss_data.csv'\n",
    "print(\"数据读取成功\")\n",
    "# 创建用户和电影映射字典,将稀疏数据稠密化\n",
    "user_ids = ratings['userId'].unique()\n",
    "user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
    "movie_ids = ratings['movieId'].unique()\n",
    "movie_to_idx = {movie: idx for idx, movie in enumerate(movie_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a2ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为连续索引\n",
    "ratings['user_idx'] = ratings['userId'].map(user_to_idx)\n",
    "ratings['movie_idx'] = ratings['movieId'].map(movie_to_idx)\n",
    "# 归一化评分到0-1范围\n",
    "ratings['rating'] = ratings['rating'] / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf291b",
   "metadata": {},
   "source": [
    "#### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ab3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "train_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a6429e",
   "metadata": {},
   "source": [
    "#### 实现Dataset类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08fdcffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Dataset类\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, ratings):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.users[idx], dtype=torch.long),\n",
    "            torch.tensor(self.movies[idx], dtype=torch.long),\n",
    "            torch.tensor(self.ratings[idx], dtype=torch.float)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e202d1bb",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d75fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(75039), tensor(3064), tensor(0.9000))\n",
      "数据加载成功\n"
     ]
    }
   ],
   "source": [
    "# 创建数据加载器\n",
    "batch_size = 4096\n",
    "\n",
    "train_dataset = MovieLensDataset(train_df['user_idx'].values, \n",
    "                               train_df['movie_idx'].values,\n",
    "                               train_df['rating'].values)\n",
    "val_dataset = MovieLensDataset(val_df['user_idx'].values,\n",
    "                             val_df['movie_idx'].values,\n",
    "                             val_df['rating'].values)\n",
    "test_dataset = MovieLensDataset(test_df['user_idx'].values,\n",
    "                              test_df['movie_idx'].values,\n",
    "                              test_df['rating'].values)\n",
    "print(train_dataset[0])  # 打印第一个样本以验证数据集\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,pin_memory=True)\n",
    "print(\"数据加载成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c1ea6",
   "metadata": {},
   "source": [
    "#### 定义推荐模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f58e6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义推荐模型\n",
    "class Recommender(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_emb = nn.Embedding(num_movies, embedding_dim)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, user, movie):\n",
    "        user_emb = self.user_emb(user)\n",
    "        movie_emb = self.movie_emb(movie)\n",
    "        x = torch.cat([user_emb, movie_emb], dim=1).to(device)\n",
    "        return self.fc(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54451cfa",
   "metadata": {},
   "source": [
    "#### 加载模型，指定优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d90a8039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender(\n",
      "  (user_emb): Embedding(200948, 64)\n",
      "  (movie_emb): Embedding(84432, 64)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (6): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_users = len(user_ids)\n",
    "n_movies = len(movie_ids)\n",
    "\n",
    "model = Recommender(n_users, n_movies).to(device)\n",
    "print(model)\n",
    "if os.path.exists('best_model.pth'):\n",
    "    model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "    print(\"已加载保存的模型参数\")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcc2093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a7f564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training: 100%|██████████| 5626/5626 [11:43<00:00,  7.99it/s]\n",
      "Epoch 1 Validation: 100%|██████████| 626/626 [00:59<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Loss: 0.0304 | Val Loss: 0.0289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 5626/5626 [11:40<00:00,  8.03it/s]\n",
      "Epoch 2 Validation: 100%|██████████| 626/626 [01:01<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Loss: 0.0279 | Val Loss: 0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 5626/5626 [11:36<00:00,  8.07it/s]\n",
      "Epoch 3 Validation: 100%|██████████| 626/626 [01:01<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Loss: 0.0269 | Val Loss: 0.0271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 5626/5626 [11:36<00:00,  8.07it/s]\n",
      "Epoch 4 Validation: 100%|██████████| 626/626 [00:59<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train Loss: 0.0261 | Val Loss: 0.0271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 5626/5626 [11:37<00:00,  8.06it/s]\n",
      "Epoch 5 Validation: 100%|██████████| 626/626 [00:58<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train Loss: 0.0253 | Val Loss: 0.0268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training:  32%|███▏      | 1776/5626 [03:39<07:55,  8.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Training\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     11\u001b[0m     user, movie, rating \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\APP\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36mMovieLensDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 13\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musers[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[0;32m     14\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmovies[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong),\n\u001b[0;32m     15\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratings[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m     16\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练循环\n",
    "epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "loss_array = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} Training'):\n",
    "        user, movie, rating = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(user, movie)\n",
    "        loss = criterion(pred, rating)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * user.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f'Epoch {epoch+1} Validation'):\n",
    "            user, movie, rating = [x.to(device) for x in batch]\n",
    "            pred = model(user, movie)\n",
    "            val_loss += criterion(pred, rating).item() * user.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    loss_array.append([train_loss,val_loss])\n",
    "\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # 保存损失数据到CSV文件\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['train_loss', 'val_loss'])  # 添加表头\n",
    "        csv_writer.writerows(loss_array)\n",
    "\n",
    "print(\"训练完成，开始测试阶段\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582e321",
   "metadata": {},
   "source": [
    "#### 测试阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ffab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        user, movie, rating = [x.to(device) for x in batch]\n",
    "        pred = model(user, movie)\n",
    "        test_loss += criterion(pred, rating).item() * user.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "test_loss_csv = 'test_loss.csv'\n",
    "\n",
    "with open(test_loss_csv, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['test_loss'])  # 添加表头\n",
    "    csv_writer.writerow([test_loss])  # 写入测试损失\n",
    "\n",
    "print(loss_array)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test RMSE: {np.sqrt(test_loss * 5.0**2):.4f}')  # 反归一化后计算RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf08444",
   "metadata": {},
   "source": [
    "#### 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f23946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import  csv\n",
    "# 直接使用当前工作目录\n",
    "csv_path = 'loss_data.csv'\n",
    "\n",
    "# Read train_loss and test_loss from loss_data.csv\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "with open(csv_path, 'r', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        train_loss.append(float(row['train_loss']))\n",
    "        val_loss.append(float(row['val_loss']))\n",
    "\n",
    "# Create epochs array\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8,6))\n",
    "# plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs,[tl* 5.0**2 for tl in train_loss],'p-',label=\"Train RMSE\")\n",
    "\n",
    "# plt.plot(epochs, test_loss, 'r-',label='val Loss')\n",
    "plt.plot(epochs,[tl* 5.0**2 for tl in val_loss],'p-',label=\"Validating RMSE\")\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Training and Validating Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "plt.ylabel(ylabel='RMSE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e02a06",
   "metadata": {},
   "source": [
    "## 添加BERT模块对标题进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057edbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8dbb57",
   "metadata": {},
   "source": [
    "#### 添加BERT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "# 加载BERT tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "bert_model.eval()  # 推理模式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c5e8f",
   "metadata": {},
   "source": [
    "#### 定义电影名称转BERT向量的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def title_to_bert_vec(title):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(title, return_tensors='pt', truncation=True, max_length=32).to(device)\n",
    "        outputs = bert_model(**inputs)\n",
    "        # 取[CLS]向量作为整体表示\n",
    "        cls_vec = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    return cls_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd58252",
   "metadata": {},
   "source": [
    "#### 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38589ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "ratings = pd.read_csv('../Dataset/ml-32m/ratings.csv')\n",
    "csv_file_path = 'loss_data.csv'\n",
    "print(\"数据读取成功\")\n",
    "# 创建用户和电影映射字典,将稀疏数据稠密化\n",
    "user_ids = ratings['userId'].unique()\n",
    "user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
    "movie_ids = ratings['movieId'].unique()\n",
    "movie_to_idx = {movie: idx for idx, movie in enumerate(movie_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6315ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为连续索引\n",
    "ratings['user_idx'] = ratings['userId'].map(user_to_idx)\n",
    "ratings['movie_idx'] = ratings['movieId'].map(movie_to_idx)\n",
    "# 归一化评分到0-1范围\n",
    "ratings['rating'] = ratings['rating'] / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed409a1e",
   "metadata": {},
   "source": [
    "#### 生成BERT向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cce04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('../Dataset/ml-32m/movies.csv')\n",
    "movieid2bertvec = {\n",
    "    row['movieId']: title_to_bert_vec(row['title'])\n",
    "    for _, row in movies.iterrows()\n",
    "}\n",
    "bert_dim = next(iter(movieid2bertvec.values())).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15892290",
   "metadata": {},
   "source": [
    "#### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19990504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "train_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7913138",
   "metadata": {},
   "source": [
    "#### 修改Dataset，返回BERT向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a4efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, users, movies, ratings, movie_ids, movieid2bertvec):\n",
    "        self.users = users\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "        self.movie_ids = movie_ids # movie_ids = movie_to_idx\n",
    "        self.movieid2bertvec = movieid2bertvec\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        movie_id = self.movie_ids[idx]\n",
    "        bert_vec = self.movieid2bertvec[movie_id]\n",
    "        return (\n",
    "            torch.tensor(self.users[idx], dtype=torch.long).to(device),\n",
    "            torch.tensor(self.movies[idx], dtype=torch.long).to(device),\n",
    "            torch.tensor(self.ratings[idx], dtype=torch.float).to(device),\n",
    "            torch.tensor(bert_vec, dtype=torch.float).to(device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d2b8dc",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c896bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据加载器\n",
    "batch_size = 2048\n",
    "\n",
    "train_dataset = MovieLensDataset(train_df['user_idx'].values, \n",
    "                               train_df['movie_idx'].values,\n",
    "                               train_df['rating'].values,\n",
    "                               train_df[\"movieId\"].values,\n",
    "                               movieid2bertvec)\n",
    "val_dataset = MovieLensDataset(val_df['user_idx'].values,\n",
    "                             val_df['movie_idx'].values,\n",
    "                             val_df['rating'].values,\n",
    "                             train_df[\"movieId\"].values,\n",
    "                             movieid2bertvec)\n",
    "test_dataset = MovieLensDataset(test_df['user_idx'].values,\n",
    "                              test_df['movie_idx'].values,\n",
    "                              test_df['rating'].values,\n",
    "                              train_df[\"movieId\"].values,\n",
    "                              movieid2bertvec)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,pin_memory=True)\n",
    "print(\"数据加载成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855fe90",
   "metadata": {},
   "source": [
    "#### 定义推荐模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim=64, hidden_dim=128, bert_dim=768):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, embedding_dim)\n",
    "        self.movie_emb = nn.Embedding(num_movies, embedding_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            # 两个emb宽度加上bert_vec的宽度\n",
    "            nn.Linear(2*embedding_dim + bert_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim//4, 1),\n",
    "            \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, user, movie, bert_vec):\n",
    "        user_emb = self.user_emb(user)\n",
    "        movie_emb = self.movie_emb(movie)\n",
    "        x = torch.cat([user_emb, movie_emb, bert_vec], dim=1)\n",
    "        return self.fc(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b59a6",
   "metadata": {},
   "source": [
    "#### 加载模型，指定优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 初始化模型和优化器\n",
    "\n",
    "n_users = len(user_ids)\n",
    "n_movies = len(movie_ids)\n",
    "\n",
    "model = Recommender(n_users, n_movies).to(device)\n",
    "if os.path.exists('best_model.pth'):\n",
    "    model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "    print(\"已加载保存的模型参数\")\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44593ec0",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd943f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "epochs = 30\n",
    "best_val_loss = float('inf')\n",
    "loss_array = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} Training'):\n",
    "        user, movie, rating, bert_vec = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(user, movie, bert_vec)\n",
    "        loss = criterion(pred, rating)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * user.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f'Epoch {epoch+1} Validation'):\n",
    "            user, movie, rating = [x.to(device) for x in batch]\n",
    "            pred = model(user, movie)\n",
    "            val_loss += criterion(pred, rating).item() * user.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    loss_array.append([train_loss,val_loss])\n",
    "\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # 保存损失数据到CSV文件\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['train_loss', 'val_loss'])  # 添加表头\n",
    "        csv_writer.writerows(loss_array)\n",
    "\n",
    "print(\"训练完成，开始测试阶段\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48596a04",
   "metadata": {},
   "source": [
    "#### 测试阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        user, movie, rating, bert_vec = [x.to(device) for x in batch]\n",
    "        pred = model(user, movie, bert_vec)\n",
    "        test_loss += criterion(pred, rating).item() * user.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "test_loss_csv = 'test_loss.csv'\n",
    "\n",
    "with open(test_loss_csv, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['test_loss'])  # 添加表头\n",
    "    csv_writer.writerow([test_loss])  # 写入测试损失\n",
    "\n",
    "print(loss_array)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test RMSE: {np.sqrt(test_loss * 5.0**2):.4f}')  # 反归一化后计算RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6044f5",
   "metadata": {},
   "source": [
    "#### 数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b705af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import  csv\n",
    "import os\n",
    "# 直接使用当前工作目录\n",
    "csv_path = 'loss_data.csv'\n",
    "\n",
    "# Read train_loss and test_loss from loss_data.csv\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "with open(csv_path, 'r', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        train_loss.append(float(row['train_loss']))\n",
    "        val_loss.append(float(row['val_loss']))\n",
    "\n",
    "# Create epochs array\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8,6))\n",
    "# plt.plot(epochs, train_loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs,[tl* 5.0**2 for tl in train_loss],'p-',label=\"Train RMSE\")\n",
    "\n",
    "# plt.plot(epochs, test_loss, 'r-',label='val Loss')\n",
    "plt.plot(epochs,[tl* 5.0**2 for tl in val_loss],'p-',label=\"Validating RMSE\")\n",
    "\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Training and Validating Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "plt.ylabel(ylabel='RMSE')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
